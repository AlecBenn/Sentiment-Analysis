# Results
This file contains an overview of the process that the Jupyter Notebook goes through and the results it provides.
## Contents
- Feature Extraction
- Fine Tuning
- Specialised Model Fine Tuned by Tarnformnet
- Limitations and further work

## Feature Extraction
To be able to classify the posts as either Bullish or Bearish, I first use feature extraction of DistilBERT. DistilBERT is an encoder model rooted in the transformative architecture which gains its language understanding prowess through pretraining on an extensive text corpus. In this pretraining it adeptly grasps linguistic structures and semantics by employing the self-attention mechanism. I first tokenise the dataset, then by using an attention mask on the input ids this ensures the model only focuses on the relevant parts of the input ids. These are then fed into DistilBERT and processed through its layers. The last hidden states, which contain the richest representation of the input text, are then used as an input to train a logistic regression which learn to map the text to ‘bullish’ or ‘bearish’ labels. Here, the L2 penalty in the logit model is used as it helps prevent overfitting by discouraging large weight values. Additionally, the inverse of the regularisation strength (C) is set to 1, which is a moderate level, and helps prevent overfitting again. 
To evaluate the quality of the model I investigate the accuracy and F1 score. The accuracy is evaluating the measure of all the correctly identified cases whereas the F1 score is the harmonic mean of Precision and Recall and puts more weight of the incorrectly classified cases. The accuracy and F1 score achieved after the feature extraction are both 72%. To truly understand the model’s performance, we also examine a confusion matrix, which shows that 30% of instances belonging to Bullish were classified as Bearish, and 25% of instances belonging to Bearish were classified as Bullish. These false positives and negatives suggest that there is some degree of misclassification between the two classes. 

## Fine tuning
While feature extraction provides us with a representation of the text data, the initial model may not be fully optimised for this task. Therefore, by fine-tuning the DistilBERT model this should increase classification accuracy as the model should become more attuned to the specific patterns and unique characteristics in the dataset. 
To enable binary classification the textual labels were transformed into numerical values and then tokenised. After the pre-processing is done, I start to train the model. This is done by removing the pretraining head of DistilBERT and replacing it with a classification head that is fine-tuned for the financial text classification problem. 
The task of fine tuning hyperparameters was an iterative process to achieve the best model performance. I used an AdamW optimiser to help prevent overfitting which seemed to be a large challenge with the dataset. I limited the model to 4 Epochs as accuracy was increasing throughout but I wanted to avoid the model memorising the training data (overfitting). Furthermore, I assess the quality of the fine-tuned model in the same way as the feature extraction, and it shows that the model has an accuracy of 73% and an F1 score of 73%. This is very similar and slightly below the results achieved in feature extraction. This may be due to the relatively small dataset of 4000 data points implying that there may not be enough data to fully leverage the power of fine-tuning.
Additionally, when passing through unseen text to the fine-tuned model, it performs rather badly. This shows the difficulties associated with a small dataset and implies that the enhanced generalisation that fine-tuning should provide was not fully harnessed.  

## Specialised Pipeline
In the final approach I take, I employ Stock-Sentiment-Bert which is a fine-tuned model of FinBERT to analyse the sentiment of stocks related social media posts and messages. The model was trained on the following labels: Bearish and Bullish. I expected to find a similar accuracy as this model was fine trained on a much larger dataset than what I had available, but the data I was passing it was unseen. When implementing the pipeline on the test dataset, the model returns an accuracy of 48% and an F1-score of 47%. This is much lower than in the other approaches. This may have been due to my fine-tuning suffering from overfitting and performing much better than expected. It also suggests that the models, without a ‘neutral’ sentiment, struggle to understand the more neutral tones and instead classify them as ‘bullish’. This can be seen in the visual representation of the pie chart which shows that the fine-tuned model classifies many more tweets as Bullish. Furthermore, the pre-trained model was using an Adam optimiser and without the weight decay that AdamW uses, it is more prone to overfitting problem which implies that this model is also not very generalisable. 

## Limitations and further work
The idea of this project was to be able to label tweets and thoughts of investors into bullish or bearish and use that as a variable to try and improve stock price predictions. It must be clear that here I was modelling text sentiment and not market sentiment so the usefulness of the labels in truly predicting the stock price is unclear. 
Another limitation is the sample size. With a lack of computational power, the sample size was capped at 4000 tweets which limits the generalisation of the fine-tuned model and leads to overfitting as the model becomes overly specialised to the training data and performs poorly on new data.
Furthermore, by only using the ‘labelled’ data, I limited myself to classifying tweets to only bullish and bearish. Whereas some of the tweets may have been very neutral in sentiment but the investor themselves felt bullish. These misleading labels may cause the model to learn incorrect associations between the text and the labels. To address this, I could have assumed that the tweets without labels are neutral in sentiment as the user did not enter a sentiment themselves. Nevertheless, it would be interesting to group and evaluate the overall sentiments by the date of the post, and then see if it maps to changes in the $AAPL stock price.
